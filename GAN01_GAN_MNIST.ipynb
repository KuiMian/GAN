{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "800459d8",
   "metadata": {},
   "source": [
    "**Using a GAN with 2 FC layers to generate MINST Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d0ca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms as T\n",
    "\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9664d68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 28\n",
    "img_channels = 1\n",
    "z_dim = 100\n",
    "\n",
    "n_hidden = 128\n",
    "n_classes = 10\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "lr = 2e-4\n",
    "k= 20 # G_lr/D_lr\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bf53dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MNIST(root=\"./data\", train=True, download=True, transform=T.Compose(\n",
    "        [\n",
    "            T.ToTensor(), \n",
    "            T.Normalize((0.5,), (0.5,)),\n",
    "        ]\n",
    "    ))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "n_samples = len(train_loader.dataset) # type: ignore\n",
    "n_batches = len(train_loader)\n",
    "print(f\"Number of training samples: {n_samples}\")\n",
    "print(f\"Number of batches: {n_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc71dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = next(iter(train_loader))\n",
    "print(f\"Image batch shape: {imgs.shape}\")\n",
    "print(f\"Label batch shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf978dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(imgs):\n",
    "    return imgs * 0.5 + 0.5\n",
    "\n",
    "def show_images(imgs, grid_size=5):\n",
    "    imgs = denormalize(imgs).squeeze(1).numpy()\n",
    "    num_imgs = min(len(imgs), grid_size*grid_size)\n",
    "    imgs = imgs[:num_imgs]\n",
    "\n",
    "    img_h, img_w = imgs.shape[1], imgs.shape[2]\n",
    "    # 拼成 grid\n",
    "    grid_imgs = np.block([[imgs[i*grid_size + j] for j in range(grid_size)] for i in range(grid_size)])\n",
    "\n",
    "    fig = px.imshow(grid_imgs, color_continuous_scale='gray', aspect='')\n",
    "    fig.update_xaxes(showticklabels=False).update_yaxes(showticklabels=False)\n",
    "    fig.update_layout(title=\"MNIST Images\", coloraxis_showscale=False)\n",
    "    fig.show()\n",
    "\n",
    "show_images(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe896c70",
   "metadata": {},
   "source": [
    "![image.png](img4ipynb/show_data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0fe793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, n_hidden, img_size, img_channels):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(z_dim, n_hidden),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(n_hidden, img_size*img_size*img_channels),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.img_size = img_size\n",
    "        self.img_channels = img_channels\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.net(z).view(-1, self.img_channels, self.img_size, self.img_size)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_size, img_channels, n_hidden):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(img_size*img_size*img_channels, n_hidden),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(n_hidden, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.img_size = img_size\n",
    "        self.img_channels = img_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.img_size*self.img_size*self.img_channels)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df693d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_D(D, D_optimizer, criterion, real_imgs, fake_imgs):\n",
    "\n",
    "    real_labels = torch.ones(real_imgs.size(0), 1).to(device)\n",
    "    fake_labels = torch.zeros(fake_imgs.size(0), 1).to(device)\n",
    "\n",
    "    real_preds = D(real_imgs)\n",
    "    real_loss = criterion(real_preds, real_labels)\n",
    "    fake_preds = D(fake_imgs.detach())\n",
    "    fake_loss = criterion(fake_preds, fake_labels)\n",
    "\n",
    "    D_loss = real_loss + fake_loss\n",
    "    D_optimizer.zero_grad()\n",
    "    D_loss.backward()\n",
    "    D_optimizer.step()\n",
    "\n",
    "    return D_loss.item(), torch.mean(real_preds).item(), torch.mean(fake_preds).item()\n",
    "\n",
    "def train_G(G, D, G_optimizer, criterion, fake_imgs):\n",
    "    real_labels = torch.ones(fake_imgs.size(0), 1).to(device)\n",
    "\n",
    "    preds = D(fake_imgs)\n",
    "    G_loss = criterion(preds, real_labels)\n",
    "\n",
    "    G_optimizer.zero_grad()\n",
    "    G_loss.backward()\n",
    "    G_optimizer.step()\n",
    "\n",
    "    return G_loss.item()\n",
    "\n",
    "def fit(D, G, D_optimizer: torch.optim.Adam, G_optimizer: torch.optim.Adam, scheduler, criterion, n_epochs):\n",
    "\n",
    "    train_history = {\n",
    "        \"D_loss\": [],\n",
    "        \"G_loss\": [],\n",
    "        \"D_real\": [],\n",
    "        \"D_fake\": [],\n",
    "        \"lr\": [],\n",
    "    }\n",
    "\n",
    "    for epoch in trange(n_epochs, desc=\"Epoch\"):\n",
    "        D_loss, G_loss, real_score, fake_score = 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "        for real_imgs, _ in train_loader:\n",
    "            real_imgs = real_imgs.to(device)\n",
    "\n",
    "            z = torch.randn(batch_size, z_dim).to(device)\n",
    "            fake_imgs = G(z)\n",
    "\n",
    "            _D_loss, _real_score, _fake_score = train_D(D, D_optimizer, criterion, real_imgs, fake_imgs)\n",
    "            D_loss += _D_loss\n",
    "            real_score += _real_score\n",
    "            fake_score += _fake_score\n",
    "\n",
    "            _G_loss = train_G(G, D, G_optimizer, criterion, fake_imgs)\n",
    "            G_loss += _G_loss\n",
    "\n",
    "        train_history[\"D_loss\"].append(D_loss / n_samples)\n",
    "        train_history[\"G_loss\"].append(G_loss / n_samples)\n",
    "        train_history[\"D_real\"].append(real_score / n_batches)\n",
    "        train_history[\"D_fake\"].append(fake_score / n_batches)\n",
    "        train_history[\"lr\"].append(scheduler.get_last_lr()[0])\n",
    "\n",
    "        G_optimizer.param_groups[0]['lr'] = train_history[\"lr\"][-1] * k\n",
    "\n",
    "        if epoch == 0 or (epoch+1) % 10 == 0:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            tqdm.write(\n",
    "                f\"Epoch [{epoch+1}/{n_epochs}] lr: {train_history['lr'][-1]:.6f} \"\n",
    "                f\"D_loss: {train_history['D_loss'][-1]:.4f} G_loss: {train_history['G_loss'][-1]:.4f} \"\n",
    "                f\"D_real: {train_history['D_real'][-1]:.4f} D_fake: {train_history['D_fake'][-1]:.4f} \"\n",
    "            )\n",
    "\n",
    "            z = torch.randn(batch_size, z_dim).to(device)\n",
    "            fake_imgs = G(z).detach().cpu()\n",
    "            show_images(fake_imgs)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "    return train_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d751963c",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = Discriminator(img_size, img_channels, n_hidden).to(device)\n",
    "G = Generator(z_dim, n_hidden, img_size, img_channels).to(device)\n",
    "\n",
    "optim_D = torch.optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optim_G = torch.optim.Adam(G.parameters(), lr=lr*k, betas=(0.5, 0.999))\n",
    "\n",
    "scheduler = lr_scheduler.LambdaLR(optim_D, lr_lambda=lambda epoch: np.exp(-0.1*epoch))\n",
    "criterion = nn.BCELoss(reduction='sum')\n",
    "\n",
    "train_history = fit(D, G, optim_D, optim_G, scheduler, criterion, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39420c53",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"img4ipynb/last_epoch.png\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2010bbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = make_subplots(rows=1, cols=3, subplot_titles=(\"Loss\", \"D(x) & D(G(z))\", \"Learning Rate\"))\n",
    "\n",
    "fig.add_traces([\n",
    "    go.Scatter(y=train_history[\"D_loss\"], mode='lines', name='D_loss'),\n",
    "    go.Scatter(y=train_history[\"G_loss\"], mode='lines', name='G_loss'),\n",
    "], rows=1, cols=1)\n",
    "\n",
    "fig.add_traces([\n",
    "    go.Scatter(y=train_history[\"D_real\"], mode='lines', name='D(x)'),\n",
    "    go.Scatter(y=train_history[\"D_fake\"], mode='lines', name='D(G(z))'),\n",
    "], rows=1, cols=2)\n",
    "\n",
    "fig.add_hline(\n",
    "    y=0.5,\n",
    "    line=dict(color=\"black\", dash=\"dash\"),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(y=train_history[\"lr\"], mode='lines', name='Learning Rate'),\n",
    "    row=1, col=3\n",
    ")\n",
    "\n",
    "fig.update_layout(height=400, width=1200, title_text=\"GAN Training History\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008a0d01",
   "metadata": {},
   "source": [
    "![train_history](img4ipynb/train_history.png)|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
